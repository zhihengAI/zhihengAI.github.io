[{"categories":["大模型"],"content":"\r","date":"2024-08-30 17:27:26","objectID":"/posts/3%E5%88%86%E9%92%9F%E5%88%A9%E7%94%A8ai%E5%88%B6%E4%BD%9Cppt/:1:0","tags":["LLM"],"title":"3分钟利用AI制作PPT","uri":"/posts/3%E5%88%86%E9%92%9F%E5%88%A9%E7%94%A8ai%E5%88%B6%E4%BD%9Cppt/"},{"categories":["大模型"],"content":"1. 生成markdown源码\r在GPT（其它大模型也一样）处上传你需要做成PPT的资料（有的话），然后输入你的需求。这里只是简单的演示，你完全可以根据自己的制作需求写prompt，如：资料里面的所有内容都要出现在PPT里面。 注意，它可能生成的md格式前面是这样，需要将红框内的删除，调整格式。 ","date":"2024-08-30 17:27:26","objectID":"/posts/3%E5%88%86%E9%92%9F%E5%88%A9%E7%94%A8ai%E5%88%B6%E4%BD%9Cppt/:1:1","tags":["LLM"],"title":"3分钟利用AI制作PPT","uri":"/posts/3%E5%88%86%E9%92%9F%E5%88%A9%E7%94%A8ai%E5%88%B6%E4%BD%9Cppt/"},{"categories":["大模型"],"content":"2. 到网站生成PPT\r到Mindshow上，点击左侧导入（Import）选项，粘贴从GPT处生成的md源码来生成PPT ","date":"2024-08-30 17:27:26","objectID":"/posts/3%E5%88%86%E9%92%9F%E5%88%A9%E7%94%A8ai%E5%88%B6%E4%BD%9Cppt/:1:2","tags":["LLM"],"title":"3分钟利用AI制作PPT","uri":"/posts/3%E5%88%86%E9%92%9F%E5%88%A9%E7%94%A8ai%E5%88%B6%E4%BD%9Cppt/"},{"categories":["大模型"],"content":"3. 选择PPT模板\r生成后会让你选择模板，这里选择免费的，想要精美的自己随意。 注意这里先选择免费模板，接着点击布局选项卡，每一页都要选择免费的模板，也就是右上角没有V+图标的，否则需要收费。 ","date":"2024-08-30 17:27:26","objectID":"/posts/3%E5%88%86%E9%92%9F%E5%88%A9%E7%94%A8ai%E5%88%B6%E4%BD%9Cppt/:1:3","tags":["LLM"],"title":"3分钟利用AI制作PPT","uri":"/posts/3%E5%88%86%E9%92%9F%E5%88%A9%E7%94%A8ai%E5%88%B6%E4%BD%9Cppt/"},{"categories":["大模型"],"content":"4. 导出PPT\r然后点击网站右上角的下载即可。 ","date":"2024-08-30 17:27:26","objectID":"/posts/3%E5%88%86%E9%92%9F%E5%88%A9%E7%94%A8ai%E5%88%B6%E4%BD%9Cppt/:1:4","tags":["LLM"],"title":"3分钟利用AI制作PPT","uri":"/posts/3%E5%88%86%E9%92%9F%E5%88%A9%E7%94%A8ai%E5%88%B6%E4%BD%9Cppt/"},{"categories":["大模型"],"content":"\r其实评测也是很经典的一套流程，读取评测数据，弄成想要的格式-\u003e加载自己的模型-\u003e将数据的问题过大模型，拿到输出-\u003e将这些输出计算你想要的指标。这里的指标对应到音频大模型caption的话就是Bleu_1, Bleu_2, Bleu_3, Bleu_4, METEOR, ROUGE_L, CIDEr, SPICE。 这里拿 Qwen-Audio 的评测代码做codebase。说明放在代码注释里面了。 ","date":"2024-08-16 22:39:57","objectID":"/posts/%E9%9F%B3%E9%A2%91%E5%A4%A7%E6%A8%A1%E5%9E%8Bcaption%E8%AF%84%E6%B5%8B%E6%AD%A5%E9%AA%A4/:1:0","tags":["LLM"],"title":"音频大模型caption评测步骤","uri":"/posts/%E9%9F%B3%E9%A2%91%E5%A4%A7%E6%A8%A1%E5%9E%8Bcaption%E8%AF%84%E6%B5%8B%E6%AD%A5%E9%AA%A4/"},{"categories":["大模型"],"content":"1. 加载相应的库\rimport argparse import itertools import json import os import random import time from functools import partial from metrics import CocoTokenizer from caption_evaluation_tools.coco_caption.pycocoevalcap.bleu.bleu import Bleu from caption_evaluation_tools.coco_caption.pycocoevalcap.meteor.meteor import Meteor from caption_evaluation_tools.coco_caption.pycocoevalcap.rouge.rouge import Rouge from caption_evaluation_tools.coco_caption.pycocoevalcap.cider.cider import Cider from caption_evaluation_tools.coco_caption.pycocoevalcap.spice.spice import Spice import torch from config import * import sys from tqdm import tqdm from transformers import AutoModelForCausalLM, AutoTokenizer import 你自己的模型 注意，这里的caption_evaluation_tools库需要从这里下载https://github.com/audio-captioning/caption-evaluation-tools ","date":"2024-08-16 22:39:57","objectID":"/posts/%E9%9F%B3%E9%A2%91%E5%A4%A7%E6%A8%A1%E5%9E%8Bcaption%E8%AF%84%E6%B5%8B%E6%AD%A5%E9%AA%A4/:1:1","tags":["LLM"],"title":"音频大模型caption评测步骤","uri":"/posts/%E9%9F%B3%E9%A2%91%E5%A4%A7%E6%A8%A1%E5%9E%8Bcaption%E8%AF%84%E6%B5%8B%E6%AD%A5%E9%AA%A4/"},{"categories":["大模型"],"content":"2. 加载模型\r# 多卡初始化 torch.distributed.init_process_group( backend='nccl', world_size=int(os.getenv('WORLD_SIZE', '1')), rank=int(os.getenv('RANK', '0')), ) torch.cuda.set_device(int(os.getenv('LOCAL_RANK', 0))) prompt = '' # 传入的参数 parser = argparse.ArgumentParser(description='val parameters') parser.add_argument('--checkpoint', type=str, default='') parser.add_argument('--tokenizer', type=str, default='') parser.add_argument('--mode', type=str, default='validate') parser.add_argument('--dataset', type=str, default='clotho') parser.add_argument('--batch_size', type=int, default=1) parser.add_argument('--num_workers', type=int, default=1) parser.add_argument('--seed', type=int, default=42) # .... args = parser.parse_args() # 如果你的模型加载方式不是这样，就改成自己加载模型的代码就行 model = AutoModelForCausalLM.from_pretrained(args.checkpoint, device_map='cuda', trust_remote_code=True).eval() tokenizer = AutoTokenizer.from_pretrained(args.tokenizer, trust_remote_code=True) tokenizer.padding_side = 'left' tokenizer.pad_token_id = tokenizer.eod_id ","date":"2024-08-16 22:39:57","objectID":"/posts/%E9%9F%B3%E9%A2%91%E5%A4%A7%E6%A8%A1%E5%9E%8Bcaption%E8%AF%84%E6%B5%8B%E6%AD%A5%E9%AA%A4/:1:2","tags":["LLM"],"title":"音频大模型caption评测步骤","uri":"/posts/%E9%9F%B3%E9%A2%91%E5%A4%A7%E6%A8%A1%E5%9E%8Bcaption%E8%AF%84%E6%B5%8B%E6%AD%A5%E9%AA%A4/"},{"categories":["大模型"],"content":"3. 加载数据集\rds_collections = { # style: clotho, audiocaps, or plain 'clotho': {'path': 'path/to/clotho_eval.jsonl', 'style': 'clotho'} } class AudioDataset(torch.utils.data.Dataset): def __init__(self, ds, prompt): path = ds['path'] self.output_style = ds['style'] self.datas = open(path).readlines() self.prompt = prompt def __len__(self): return len(self.datas) def __getitem__(self, idx): data = json.loads(self.datas[idx].strip()) audio_path = data['audio'] source = data['source'] gt = data['gt'] return { 'input_text': self.prompt.format(audio_path, self.output_style), 'audio_path': audio_path, 'source': source, 'gt': gt } # 取一批次的数据的函数 def collate_fn(inputs, tokenizer): input_texts = [_['input_text'] for _ in inputs] source = [_['source'] for _ in inputs] gt = [_['gt'] for _ in inputs] audio_path = [_['audio_path'] for _ in inputs] audio_info = [tokenizer.process_audio(_['input_text']) for _ in inputs] input_tokens = tokenizer(input_texts, return_tensors='pt', padding='longest', audio_info=audio_info) return input_tokens.input_ids, input_tokens.attention_mask, source, gt, audio_path, audio_info # 分布式数据分片分发 class InferenceSampler(torch.utils.data.sampler.Sampler): def __init__(self, size): self._size = int(size) assert size \u003e 0 self._rank = torch.distributed.get_rank() self._world_size = torch.distributed.get_world_size() self._local_indices = self._get_local_indices(size, self._world_size, self._rank) @staticmethod def _get_local_indices(total_size, world_size, rank): shard_size = total_size // world_size left = total_size % world_size shard_sizes = [shard_size + int(r \u003c left) for r in range(world_size)] begin = sum(shard_sizes[:rank]) end = min(sum(shard_sizes[:rank + 1]), total_size) return range(begin, end) def __iter__(self): yield from self._local_indices def __len__(self): return len(self._local_indices) random.seed(args.seed) dataset = AudioDataset( ds=ds_collections[args.dataset], prompt=prompt ) # data_loader，作用不必多说 data_loader = torch.utils.data.DataLoader( dataset=dataset, sampler=InferenceSampler(len(dataset)), batch_size=args.batch_size, num_workers=args.num_workers, pin_memory=True, drop_last=False, collate_fn=partial(collate_fn, tokenizer=tokenizer), ) ","date":"2024-08-16 22:39:57","objectID":"/posts/%E9%9F%B3%E9%A2%91%E5%A4%A7%E6%A8%A1%E5%9E%8Bcaption%E8%AF%84%E6%B5%8B%E6%AD%A5%E9%AA%A4/:1:3","tags":["LLM"],"title":"音频大模型caption评测步骤","uri":"/posts/%E9%9F%B3%E9%A2%91%E5%A4%A7%E6%A8%A1%E5%9E%8Bcaption%E8%AF%84%E6%B5%8B%E6%AD%A5%E9%AA%A4/"},{"categories":["大模型"],"content":"4. 评测数据过大模型得到输出\rgts = [] sources = [] rets = [] audio_paths = [] for _, (input_ids, attention_mask, source, gt, audio_path, audio_info) in tqdm(enumerate(data_loader), total=len(data_loader), desc='Evaluaing'): # ----------------------------------------------------------------------------- # 我的模型的逻辑是将 audio 过模型的 encoder，与 text 过 tokenizer 后的特征 cat 起来，为下面送进大模型做准备 inputs_embeds = model.prepare_embedding_for_eval( 'Generate the caption in English: ', audio_path[0]) # 这里注意了，可能你的模型不是这样生成结果的，需要改成自身模型的generate outputs = model.generate( inputs_embeds=inputs_embeds, # attention_mask=attention_mask.cuda(), max_new_tokens=30, top_p=0.1, temperature=1, do_sample=True, use_cache=True, output_hidden_states=True, return_dict_in_generate=True, output_attentions=False ) # 这里的指标计算就是拿到模型输出与 ground truth caption 做 BLEU 这些相似度计算 output_ids = outputs.sequences output_text = model.batch_decode( output_ids, skip_special_tokens=True)[0] # ----------------------------------------------------------------------------- # 需要更改的地方可能只有上面这一段了，只要拿到了输出，后面计算逻辑是一样的 eos_token_pos = output_ids.eq(tokenizer.eod_id).float().argmax(-1) # 注意，这里的 rets 里面一个 [] 是一个元素 rets.extend([output_text]) gts.extend(gt) sources.extend(source) audio_paths.extend(audio_path) 遍历完毕后分布式整合数据 torch.distributed.barrier() world_size = torch.distributed.get_world_size() merged_gts = [None for _ in range(world_size)] merged_sources = [None for _ in range(world_size)] merged_responses = [None for _ in range(world_size)] merged_audio_paths = [None for _ in range(world_size)] torch.distributed.all_gather_object(merged_gts, gts) torch.distributed.all_gather_object(merged_sources, sources) torch.distributed.all_gather_object(merged_responses, rets) torch.distributed.all_gather_object(merged_audio_paths, audio_paths) merged_gts = [_ for _ in itertools.chain.from_iterable(merged_gts)] merged_sources = [_ for _ in itertools.chain.from_iterable(merged_sources)] merged_audio_paths = [ _ for _ in itertools.chain.from_iterable(merged_audio_paths)] merged_responses = [ _ for _ in itertools.chain.from_iterable(merged_responses) ] ","date":"2024-08-16 22:39:57","objectID":"/posts/%E9%9F%B3%E9%A2%91%E5%A4%A7%E6%A8%A1%E5%9E%8Bcaption%E8%AF%84%E6%B5%8B%E6%AD%A5%E9%AA%A4/:1:4","tags":["LLM"],"title":"音频大模型caption评测步骤","uri":"/posts/%E9%9F%B3%E9%A2%91%E5%A4%A7%E6%A8%A1%E5%9E%8Bcaption%E8%AF%84%E6%B5%8B%E6%AD%A5%E9%AA%A4/"},{"categories":["大模型"],"content":"5. 开始评测\r# 在主卡上，且数据都整合好了就开始做评测 if torch.distributed.get_rank() == 0: print(f\"Evaluating {args.dataset} ...\") results = [] for gt, response, source, audio_path in zip(merged_gts, merged_responses, merged_sources, merged_audio_paths): results.append({ 'gt': gt, 'response': response, 'source': source, 'audio_path': audio_path, }) time_prefix = time.strftime('%y%m%d%H%M%S', time.localtime()) results_file = f'{args.dataset}_{time_prefix}.json' # 保存这次模型生成好的数据 json.dump(results, open(results_file, 'w', encoding='utf-8'), ensure_ascii=False) results_dict = {} for item in tqdm(results): source = item[\"source\"] results_dict.setdefault(source, []).append(item) for source in results_dict: refs, hyps = [], [] results_list = results_dict[source] for result in results_list: gt = result[\"gt\"] response = result[\"response\"] refs.append(gt) hyps.append(response) # 这里就是做评测计算的代码，是用开源的代码 # 如果是自己项目需要特定的评测，可以自己在这里改 score_dict = compute_caption(refs, hyps) # 如果做Speech Emotion Recognition ，这里的评测指标就是这个 # score = accuracy_score(refs, hyps) print(f\"source: {source}\\tcnt: {len(refs)}\\tRes: {score_dict}\") torch.distributed.barrier() 以上就是评测的步骤了，如果是其它任务的话，只需要修改最后的评测指标的计算代码就OK。 比如要评测Speech Emotion Recognition (SER)，只需要把compute_caption换成sklearn.metrics的accuracy_score函数就好了，哦对了相应的ds_collections数据名字也要修改。 运行脚本代码： # --nproc_per_node 对应你的 CUDA_VISIBLE_DEVICES 的数量 ds='clotho' CUDA_VISIBLE_DEVICES=0 python -m torch.distributed.launch --use_env \\ --nproc_per_node 1 --nnodes 1 \\ evaluate_caption.py ","date":"2024-08-16 22:39:57","objectID":"/posts/%E9%9F%B3%E9%A2%91%E5%A4%A7%E6%A8%A1%E5%9E%8Bcaption%E8%AF%84%E6%B5%8B%E6%AD%A5%E9%AA%A4/:1:5","tags":["LLM"],"title":"音频大模型caption评测步骤","uri":"/posts/%E9%9F%B3%E9%A2%91%E5%A4%A7%E6%A8%A1%E5%9E%8Bcaption%E8%AF%84%E6%B5%8B%E6%AD%A5%E9%AA%A4/"},{"categories":["大模型"],"content":"6. 遇到的错误以及解决方案\r因为这个评测指标的核心计算代码是斯坦福的NLP小组写的，他们用的是java，caption_evaluation_tools用python调用java来执行，所以遇到的错误全是 java 相关的，这里写出来。 I. 服务器安装java sudo apt-get install openjdk-8-jre-headless II. java.lang.NoClassDefFoundError: edu/stanford/nlp/semgraph/semgrex/Semgrex，具体是下面的错误 Exception in thread \"main\" java.lang.NoClassDefFoundError: edu/stanford/nlp/semgraph/semgrex/SemgrexPattern at edu.anu.spice.SpiceParser.\u003cclinit\u003e(SpiceParser.java:64) at edu.anu.spice.SpiceScorer.scoreBatch(SpiceScorer.java:70) at edu.anu.spice.SpiceScorer.main(SpiceScorer.java:60) Caused by: java.lang.ClassNotFoundException: edu.stanford.nlp.semgraph.semgrex.SemgrexPattern at java.net.URLClassLoader.findClass(URLClassLoader.java:387) at java.lang.ClassLoader.loadClass(ClassLoader.java:418) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:355) at java.lang.ClassLoader.loadClass(ClassLoader.java:351) 解决方法： 我发现源码提供的nlp jar版本是3.4.1，先去官网下载 3.6.0 的包，替换掉caption_evaluation_tools/pycocoevalcap/tokenizer/ptbtokenizer.py 下的 3.4.1为 3.6.0。 将stanford-corenlp-3.6.0.jar和stanford-corenlp-3.6.0-models.jar这两个包放进/path/to/你的环境名/lib/python3.x/site-packages/pycocoeval/spice/lib/ 这里面。 III. java.lang.NoClassDefFoundError: org/slf4j/LoggerFactory 和 java.lang.NoClassDefFoundError: org/apache/log4j/Level 两个错误 这两个错误都是一样的，缺少包了，去mvnrepository下载 SLF4J API和SLF4J LOG4J .jar文件，以及下载 Apache Log4j .jar文件，将它们放到caption_evaluation_tools/pycocoevalcap/spice/lib/ 下。 然后修改spice.py文件的 spice_cmd，加一个参数 ‘-cp’, ’lib/slf4j-api-1.7.12.jar:lib/slf4j-log4j12-1.7.12.jar:libs/jog4j-1.2.17.jar’ ","date":"2024-08-16 22:39:57","objectID":"/posts/%E9%9F%B3%E9%A2%91%E5%A4%A7%E6%A8%A1%E5%9E%8Bcaption%E8%AF%84%E6%B5%8B%E6%AD%A5%E9%AA%A4/:1:6","tags":["LLM"],"title":"音频大模型caption评测步骤","uri":"/posts/%E9%9F%B3%E9%A2%91%E5%A4%A7%E6%A8%A1%E5%9E%8Bcaption%E8%AF%84%E6%B5%8B%E6%AD%A5%E9%AA%A4/"},{"categories":["大模型"],"content":"\r","date":"2024-08-15 17:41:28","objectID":"/posts/deepspeed%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E4%B8%AD%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/:1:0","tags":["LLM"],"title":"Deepspeed多机多卡训练中配置以及出现的问题","uri":"/posts/deepspeed%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E4%B8%AD%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["大模型"],"content":"I. Deepspeed 多机多卡训练步骤\r","date":"2024-08-15 17:41:28","objectID":"/posts/deepspeed%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E4%B8%AD%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/:2:0","tags":["LLM"],"title":"Deepspeed多机多卡训练中配置以及出现的问题","uri":"/posts/deepspeed%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E4%B8%AD%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["大模型"],"content":"1. 设置训练服务器之间的ssh免密登录\r首先，在你的客户端机器上生成一个 SSH 密钥对，假设当前训练机器是：192.168.1.100, 192.168.1.101, 192.168.1.102 # 通常生成在 ~/.ssh/id_rsa ssh-keygen -t rsa -b 4096 然后将公钥复制到远程服务器 ssh-copy-id user@192.168.1.101 ssh-copy-id user@192.168.1.102 最后修改 ~/.ssh/config vim ~/.ssh/config Host 192.168.1.101 HostName 192.168.1.101 # 远程服务器的IP地址或主机名 User your_username # 登录的用户名 IdentityFile ~/.ssh/id_rsa # 使用的SSH私钥文件路径（默认是id_rsa） Port 22 # SSH端口（默认是22，可以省略） Host 192.168.1.102 HostName 192.168.1.102 # 远程服务器的IP地址或主机名 User your_username # 登录的用户名 IdentityFile ~/.ssh/id_rsa # 使用的SSH私钥文件路径（默认是id_rsa） Port 22 # SSH端口（默认是22，可以省略） 就能免密登录了。 ","date":"2024-08-15 17:41:28","objectID":"/posts/deepspeed%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E4%B8%AD%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/:2:1","tags":["LLM"],"title":"Deepspeed多机多卡训练中配置以及出现的问题","uri":"/posts/deepspeed%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E4%B8%AD%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["大模型"],"content":"2. 同步训练代码和数据\r同步方法就那几个，这里贴一个让gpt写的脚本 #!/bin/bash # 配置源代码和数据集的路径 SOURCE_DIR=\"/path/to/source/directory\" DATASET_DIR=\"/path/to/dataset/directory\" # 配置目标服务器信息 SERVERS=(\"user1@server1:/path/to/destination/\" \"user2@server2:/path/to/destination/\") for SERVER in \"${SERVERS[@]}\" do echo \"正在同步到 $SERVER\" # 同步源代码 scp -r \"$SOURCE_DIR\" \"$SERVER\" if [ $? -eq 0 ]; then echo \"源代码已成功同步到 $SERVER\" else echo \"源代码同步到 $SERVER 失败\" fi # 同步数据集 scp -r \"$DATASET_DIR\" \"$SERVER\" if [ $? -eq 0 ]; then echo \"数据集已成功同步到 $SERVER\" else echo \"数据集同步到 $SERVER 失败\" fi echo \"完成与 $SERVER 的同步\" done echo \"所有服务器的同步已完成\" ","date":"2024-08-15 17:41:28","objectID":"/posts/deepspeed%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E4%B8%AD%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/:2:2","tags":["LLM"],"title":"Deepspeed多机多卡训练中配置以及出现的问题","uri":"/posts/deepspeed%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E4%B8%AD%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["大模型"],"content":"3. 添加Deepspeed hostfile.txt\r有几台机子就写几行，前面的 IP 地址和 ~/.ssh/config 定义的一致就行。后面的 slots=4 就是每台服务器开 4 张卡。 192.168.1.100 slots=4 192.168.1.101 slots=4 192.168.1.102 slots=4 ","date":"2024-08-15 17:41:28","objectID":"/posts/deepspeed%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E4%B8%AD%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/:2:3","tags":["LLM"],"title":"Deepspeed多机多卡训练中配置以及出现的问题","uri":"/posts/deepspeed%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E4%B8%AD%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["大模型"],"content":"4. 开始训练\r这里以有3台服务器，每台服务器开4张GPU，主节点为 12.168.1.100，通信端口为12345，则我们只需要在在命令行输入： deepspeed --hostfile=/path/to/hostfile.txt --num_nodes=3 --num_gpus=4 --master_addr=\"192.168.1.100\" --master_port=12345 \\ train.py --deepspeed_config ds_config.json 只要同步完代码和训练数据集，就能跑啦（不出错的话）！ ","date":"2024-08-15 17:41:28","objectID":"/posts/deepspeed%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E4%B8%AD%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/:2:4","tags":["LLM"],"title":"Deepspeed多机多卡训练中配置以及出现的问题","uri":"/posts/deepspeed%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E4%B8%AD%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["大模型"],"content":"II. 遇到的错误\r","date":"2024-08-15 17:41:28","objectID":"/posts/deepspeed%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E4%B8%AD%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/:3:0","tags":["LLM"],"title":"Deepspeed多机多卡训练中配置以及出现的问题","uri":"/posts/deepspeed%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E4%B8%AD%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["大模型"],"content":"1. 需要排除指定训练通讯的网卡\rifconfig # 先查看通信网卡名字 export NCCL_IB_HCA=mlx5_0:1 export NCCL_IB_DISABLE=0 export NCCL_P2P_DISABLE=1 export NCCL_DEBUG=INFO export NCCL_P2P_LEVEL=NVL # 排除掉不用的网卡，留下用的，因为用来训练的服务器通信的网卡的名字可能不一样 # 只要排除掉不是这些名字的，就能通信了 export NCCL_SOCKET_IFNAME=^wangka1,wangka2,wangka3,... 不用^排除的话可能会通信不了，梯度回传失败 ","date":"2024-08-15 17:41:28","objectID":"/posts/deepspeed%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E4%B8%AD%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/:3:1","tags":["LLM"],"title":"Deepspeed多机多卡训练中配置以及出现的问题","uri":"/posts/deepspeed%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E4%B8%AD%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["大模型"],"content":"2. AttributeError: ‘DeepSpeedCPUAdam‘ object has no attribute ‘ds_opt_adam\r因为服务器不止我一个人用，还有别人的环境，cuda不好重新安装，所以可以采取修改源码的方式解决 vim /path/to/conda_env_name/lib/python3.xx/site-packages/deepspeed/ops/op_builder/builder.py if sys_cuda_version != torch_cuda_version: return True # 修改这里，直接return True回去不检查版本 if (cuda_major in cuda_minor_mismatch_ok and sys_cuda_version in cuda_minor_mismatch_ok[cuda_major] and torch_cuda_version in cuda_minor_mismatch_ok[cuda_major]): print(f\"Installed CUDA version {sys_cuda_version} does not match the \" f\"version torch was compiled with {torch.version.cuda} \" \"but since the APIs are compatible, accepting this combination\") return True elif os.getenv(\"DS_SKIP_CUDA_CHECK\", \"0\") == \"1\": print( f\"{WARNING} DeepSpeed Op Builder: Installed CUDA version {sys_cuda_version} does not match the \" f\"version torch was compiled with {torch.version.cuda}.\" \"Detected `DS_SKIP_CUDA_CHECK=1`: Allowing this combination of CUDA, but it may result in unexpected behavior.\" ) return True raise CUDAMismatchException( f\"\u003e- DeepSpeed Op Builder: Installed CUDA version {sys_cuda_version} does not match the \" f\"version torch was compiled with {torch.version.cuda}, unable to compile \" \"cuda/cpp extensions without a matching cuda version.\") return True 也可以不改源码的，可以通过设置环境变量的方式跳过cuda检查，只需要在执行代码前加上 DS_SKIP_CUDA_CHECK=1 ","date":"2024-08-15 17:41:28","objectID":"/posts/deepspeed%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E4%B8%AD%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/:3:2","tags":["LLM"],"title":"Deepspeed多机多卡训练中配置以及出现的问题","uri":"/posts/deepspeed%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E4%B8%AD%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["大模型"],"content":"3. Error building extension ‘cpu_adam’\r解决方法：https://github.com/microsoft/DeepSpeed/issues/2268#issuecomment-1230830048 把python3.x版本的安装成python3.x版本的python-dev sudo apt-get install python3.x-dev ","date":"2024-08-15 17:41:28","objectID":"/posts/deepspeed%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E4%B8%AD%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/:3:3","tags":["LLM"],"title":"Deepspeed多机多卡训练中配置以及出现的问题","uri":"/posts/deepspeed%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E4%B8%AD%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["大模型"],"content":"4. AttributeError: ‘DeepSpeedCPUAdam’ object has no attribute ‘ds_opt_adam’\r解决方法：在训练文件头部加入 import deepspeed deepspeed.ops.op_builder.CPUAdamBuilder().load() ","date":"2024-08-15 17:41:28","objectID":"/posts/deepspeed%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E4%B8%AD%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/:3:4","tags":["LLM"],"title":"Deepspeed多机多卡训练中配置以及出现的问题","uri":"/posts/deepspeed%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E4%B8%AD%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["大模型"],"content":"5. Ninja is required to load C++ extension\r解决方法：重新安装 ninja sudo apt-get install ninja-build ","date":"2024-08-15 17:41:28","objectID":"/posts/deepspeed%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E4%B8%AD%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/:3:5","tags":["LLM"],"title":"Deepspeed多机多卡训练中配置以及出现的问题","uri":"/posts/deepspeed%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E4%B8%AD%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["大模型"],"content":"6. py310_cu117/fused_adam/fused_adam.so: cannot open shared object file: No such file or directory 找不到adam.so，或者其他so缺失这类问题\r重新编译安装deepspeed即可，当然要先删除缓存： rm -rf ~/.cache/torch rm -rf ~/.cache/torch_extensions pip uninstall deepspeed DS_BUILD_UTILS=1 python -m pip install deepspeed==0.xx.xx ","date":"2024-08-15 17:41:28","objectID":"/posts/deepspeed%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E4%B8%AD%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/:3:6","tags":["LLM"],"title":"Deepspeed多机多卡训练中配置以及出现的问题","uri":"/posts/deepspeed%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E4%B8%AD%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["大模型"],"content":"7. nvcc fatal : Unsupported gpu architecture ‘compute_86‘\r错误原因：有些GPU算力是 8.6，而当前 CUDA 版本 不支持算力 8.6。 解决方法：向系统环境变量 ~/.bashrc 中添加一项：TORCH_CUDA_ARCH_LIST，值为8.0（也可以改成7.5或8.6试一下）。 vim ~/.bashrc 注意：由于CUDA与硬件有关，故添加系统环境变量后要重启电脑，重新运行才会通过。 ","date":"2024-08-15 17:41:28","objectID":"/posts/deepspeed%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E4%B8%AD%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/:3:7","tags":["LLM"],"title":"Deepspeed多机多卡训练中配置以及出现的问题","uri":"/posts/deepspeed%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E4%B8%AD%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["大模型"],"content":"\r","date":"2024-08-14 22:44:35","objectID":"/posts/%E5%A4%84%E7%90%86%E5%85%AC%E5%BC%80%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%A1%B9%E7%9B%AE%E8%AE%AD%E7%BB%83%E9%9C%80%E8%A6%81%E7%9A%84%E6%A0%BC%E5%BC%8F%E5%A4%84%E7%90%86%E6%AD%A5%E9%AA%A4/:1:0","tags":["LLM"],"title":"处理公开数据集成大模型项目训练需要的格式处理步骤","uri":"/posts/%E5%A4%84%E7%90%86%E5%85%AC%E5%BC%80%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%A1%B9%E7%9B%AE%E8%AE%AD%E7%BB%83%E9%9C%80%E8%A6%81%E7%9A%84%E6%A0%BC%E5%BC%8F%E5%A4%84%E7%90%86%E6%AD%A5%E9%AA%A4/"},{"categories":["大模型"],"content":"1. 下载数据集\r这里以音频数据集为例子： CHiME-Home dataset——Sound Source Recognition in a Domestic Environment，其实就是个xx声音源识别标注数据集，一般下载下来的是.wav音频文件和.csv标注文本文件。 ","date":"2024-08-14 22:44:35","objectID":"/posts/%E5%A4%84%E7%90%86%E5%85%AC%E5%BC%80%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%A1%B9%E7%9B%AE%E8%AE%AD%E7%BB%83%E9%9C%80%E8%A6%81%E7%9A%84%E6%A0%BC%E5%BC%8F%E5%A4%84%E7%90%86%E6%AD%A5%E9%AA%A4/:1:1","tags":["LLM"],"title":"处理公开数据集成大模型项目训练需要的格式处理步骤","uri":"/posts/%E5%A4%84%E7%90%86%E5%85%AC%E5%BC%80%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%A1%B9%E7%9B%AE%E8%AE%AD%E7%BB%83%E9%9C%80%E8%A6%81%E7%9A%84%E6%A0%BC%E5%BC%8F%E5%A4%84%E7%90%86%E6%AD%A5%E9%AA%A4/"},{"categories":["大模型"],"content":"2. 写脚本处理成自己项目需要的格式\r比如我需要处理成JSON文件，且每个item的格式为： { \"audio\": \"name.wav\", \"data\": [ { \"role\": \"human\", \"value\": \"问题问题问题\" }, { \"role\": \"machine\", \"value\": \"从csv标注文件中提取的对应的答案文本\" } ] } 现在写程序应该都是写个prompt让大模型帮人写了吧，这里贴一个我写的prompt，模型我用的是GPT4o\r你是一名python专家，现在我有一个文件夹，里面有.wav文件和.csv文件，其中.csv文件是对.wav文件的标注。我需要提取.csv文件里面的信息，保存成json格式 csv格式如下： segmentname,CR_lounge_200110_1601.s0 chunknumber,0 framestart,0 annotation_a1,cv session_a1,2.0 annotation_a2,cv session_a2,4.0 annotation_a3,cv session_a3,5.0 majorityvote,cv chunkname,CR_lounge_200110_1601.s0_chunk0 我需要拿到majorityvote的值（这里也就是cv），然后构造json一个项： { \"audio\": \"CR_lounge_200110_1601.s0_chunk0.16KHz.wav\", \"data\": [ { \"role\": \"human\", \"value\": random.choice(question_seeds) }, { \"role\": \"machine\", \"value\": random.choice(answer_seeds) + translated_cv } ] } 其中translated_cv是翻译majorityvote后的结果，这里是，翻译对照如下： c 儿童说话声 m 成年男性说话声 f 成年女性说话声 v 电子游戏/电视声 p 打击声，如撞击声、撞击声、敲击声、脚步声 b 宽频噪音，例如家用电器 o 其他可识别的声音 所以 cv 就是儿童说话声和电子游戏/电视的声音 question_seeds和answer_seeds是个[]，你忽略掉就行 请你写一个python实现它，要求使用多线程，线程数为8，并且需要打印信息。 ","date":"2024-08-14 22:44:35","objectID":"/posts/%E5%A4%84%E7%90%86%E5%85%AC%E5%BC%80%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%A1%B9%E7%9B%AE%E8%AE%AD%E7%BB%83%E9%9C%80%E8%A6%81%E7%9A%84%E6%A0%BC%E5%BC%8F%E5%A4%84%E7%90%86%E6%AD%A5%E9%AA%A4/:1:2","tags":["LLM"],"title":"处理公开数据集成大模型项目训练需要的格式处理步骤","uri":"/posts/%E5%A4%84%E7%90%86%E5%85%AC%E5%BC%80%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%A1%B9%E7%9B%AE%E8%AE%AD%E7%BB%83%E9%9C%80%E8%A6%81%E7%9A%84%E6%A0%BC%E5%BC%8F%E5%A4%84%E7%90%86%E6%AD%A5%E9%AA%A4/"},{"categories":["大模型"],"content":"3. 小修代码\r代码其实直接就能用了，一次跑通。现在是要补充question_seeds和answer_seeds，是问题和答案的随机种子，因为原本的数据答案太单调，如：标注是 mv，回答只有成年男性说话声，电子游戏/电视声，所以我们要造一些回答前缀和问题。 还是老办法，估计各位也不会自己写几十条，还是要依靠GPT4o，这里也贴出我的prompt\rquestion_seeds = [ '这段音频的场景是在什么地方？', '这个音频是在什么场景录制的？', '这段录音中的背景声音让人联想到什么样的环境或场所？', '分析音频中的声音线索，你能推测出这是在哪里？', '音频中的音效内容提示了怎样的场景？', '听了这段音频后，你推断的发生地点是什么样的？', 'xxxxxxxx' ] 这是环境识别的问题种子，现在我有个新的音频任务，是识别人物和物体发声的，请你写30个给出的格式的问题，如： 这段音频有什么人物或者物体发出了声音吗？ answer_seeds也是同样的方法，最终构造出30条左右，这里我给出最终代码\rimport os import csv import json import random import concurrent.futures question_seeds = [ '这段音频中有哪些人物或者物体发出了声音？', '音频里有哪些人物或物体在发声？', '你能听出音频中的哪些人物或物体在发声吗？', '音频中出现了哪些人物或物体的声音？', '这段音频里的声音是由哪些人物或物体发出的？', '根据音频，你能识别出哪些人物或物体在发声吗？', '这段音频中有什么特定的人物或物体在发声？', '听这段音频，你能确定有哪些人物或物体在发声吗？', '这段音频中的人物或物体发出的声音是什么样的？', '音频中有哪些人物或物体在制造声音？', '这段音频里能听到哪些人物或物体的声音？', '音频中能识别出哪些人物或物体在发声吗？', '音频中有哪些人物或物体发出了噪音？', '这段音频中有哪些人物或物体的声音？', '你能分辨出音频中的哪些人物或物体在发声吗？', '音频里哪些人物或物体发出了声响？', '听音频，你能识别出哪些人物或物体在讲话吗？', '音频中哪些人物或物体发出了特殊的声响？', '音频里能听到哪些人物或物体的声音？', '音频中能辨别出哪些人物或物体在发声吗？', '这段音频中人物或物体的声音有哪些特征？', '音频中有哪些人物或物体在制造噪音？', '音频里你能听出哪些人物或物体在发声吗？', '这段音频中有哪些物体的声音和哪些人物的声音？', '音频中能识别出哪些人物或物体的声音吗？', '音频里什么人物或物体在发出声响？', '听音频，你能分辨出哪些人物或物体的声音吗？', '音频中有什么人物或物体在制造声响？', '这段音频里哪些人物或物体的声音可以被识别？', '你能从音频中辨别出哪些人物或物体在发声吗？', ] answer_seeds = [ '根据音频中的人声和物体声音特征，可以推测这段音频中有', '通过分析音频中的声音元素，可以判断这段音频中有', '从音频中的特定声音来看，这段音频中有', '根据音频捕捉到的声音，可以推测这段音频中包含', '结合音频中的声音特征，可以判断这段音频中涉及', '从音频中的声音细节来看，可以听出这段音频中有', '根据音频中的人声和物体声，可以推测这段音频中存在', '通过对音频中的声音进行分析，可以判断这段音频中有', '从音频中的声音特征分析，可以听出这段音频中包含', '根据音频中的声音线索，可以推断这段音频中有', '鉴于音频中的声音特征，可以推测这段音频中有', '从音频中的人声和物体声音来看，这段音频中包含', '结合音频中的声音元素，可以判断这段音频中有', '根据音频中的特定声音，可以推测这段音频中有', '通过分析音频中的声音细节，可以判断这段音频中包含', '从音频捕捉到的声音特征来看，这段音频中有', '根据音频中的声音特征，可以推断这段音频中有', '从音频中的声音线索来看，这段音频中有', '通过对音频中的声音进行分析，可以推测这段音频中有', '根据音频中的背景音和物体声音，可以判断这段音频中包含', '从音频中的人声和物体互动音来看，这段音频中有', '结合音频中的声音细节，可以推测这段音频中有', '根据音频中的声音内容，可以推断这段音频中有', '通过分析音频中的各种声音元素，可以判断这段音频中有', '从音频中的声音特征分析，可以推测这段音频中有', '根据音频中的人声和物体声，可以推断这段音频中有', '结合音频中的声音线索，可以判断这段音频中包含', '从音频捕捉到的声音来看，这段音频中有', '通过分析音频中的声音细节，可以推测这段音频中有', '根据音频中的背景音和互动音，可以判断这段音频中有', ] translation_dict = { 'c': '儿童说话声', 'm': '成年男性说话声', 'f': '成年女性说话声', 'v': '电子游戏/电视声', 'p': '打击声，如撞击声、敲击声、脚步声', 'b': '宽频噪音，例如家用电器发出的声音', } def translate_majorityvote(vote): translated_elements = [translation_dict[char] for char in vote if char in translation_dict] return '，'.join(translated_elements) def process_csv_file(csv_file): with open(csv_file, 'r', encoding='utf-8') as f: reader = csv.reader(f) for row in reader: if row[0] == 'majorityvote': majorityvote = row[1] if row[0] == 'chunkname': chunkname = row[1] translated_cv = translate_majorityvote(majorityvote) json_data = { \"audio\": f\"chime_home/{chunkname}.16KHz.wav\", \"data\": [ { \"role\": \"human\", \"value\": random.choice(question_seeds) }, { \"role\": \"machine\", \"value\": random.choice(answer_seeds) + translated_cv } ] } return json_data def main(folder_path, output_json_path): csv_files = [os.path.join(folder_path, file) for file in os.listdir( folder_path) if file.endswith('.csv')] json_results = [] with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor: futures = {executor.submit(process_csv_file, csv_file): csv_file for csv_file in csv_files} for future in concurrent.futures.as_completed(futures): csv_file = futures[future] try: json_data = future.result() json_results.append(json_data) print(f\"Processed {csv_file} successfully.\") except Exception as exc: print(f\"Error processing {csv_file}: {exc}\") with open(output_json_path, 'w', encoding='utf-8') as f: json.dump(json_results, f, ensure_ascii=False, separators=(',', ':')) if __name__ == \"__main__\": folder_path = \"chunks\" main(folder_path, 'chime_home.json') ","date":"2024-08-14 22:44:35","objectID":"/posts/%E5%A4%84%E7%90%86%E5%85%AC%E5%BC%80%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%A1%B9%E7%9B%AE%E8%AE%AD%E7%BB%83%E9%9C%80%E8%A6%81%E7%9A%84%E6%A0%BC%E5%BC%8F%E5%A4%84%E7%90%86%E6%AD%A5%E9%AA%A4/:1:3","tags":["LLM"],"title":"处理公开数据集成大模型项目训练需要的格式处理步骤","uri":"/posts/%E5%A4%84%E7%90%86%E5%85%AC%E5%BC%80%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%A1%B9%E7%9B%AE%E8%AE%AD%E7%BB%83%E9%9C%80%E8%A6%81%E7%9A%84%E6%A0%BC%E5%BC%8F%E5%A4%84%E7%90%86%E6%AD%A5%E9%AA%A4/"},{"categories":["大模型"],"content":"4. 总结与经验\r用处理好的数据跟其它先前弄好的不同任务的数据（大约50k条）训练了一版后，发现能改进的地方就是 answer_seeds ，如果这里的前缀种子感觉太生硬粗糙，回答的文本会很尴尬，所以需要花时间去优化随机种子。 如果需要英文版本的功能就复制源程序代码让GPT生成英文版本的就行，这样和中文数据拼一起就有双倍的数据了。 ","date":"2024-08-14 22:44:35","objectID":"/posts/%E5%A4%84%E7%90%86%E5%85%AC%E5%BC%80%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%A1%B9%E7%9B%AE%E8%AE%AD%E7%BB%83%E9%9C%80%E8%A6%81%E7%9A%84%E6%A0%BC%E5%BC%8F%E5%A4%84%E7%90%86%E6%AD%A5%E9%AA%A4/:1:4","tags":["LLM"],"title":"处理公开数据集成大模型项目训练需要的格式处理步骤","uri":"/posts/%E5%A4%84%E7%90%86%E5%85%AC%E5%BC%80%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%A1%B9%E7%9B%AE%E8%AE%AD%E7%BB%83%E9%9C%80%E8%A6%81%E7%9A%84%E6%A0%BC%E5%BC%8F%E5%A4%84%E7%90%86%E6%AD%A5%E9%AA%A4/"},{"categories":["大模型"],"content":"单独提取XX大模型中的encoder及权重","date":"2024-08-11 14:01:06","objectID":"/posts/%E5%8D%95%E7%8B%AC%E6%8F%90%E5%8F%96xx%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84encoder%E5%8F%8A%E6%9D%83%E9%87%8D/","tags":["LLM"],"title":"单独提取XX大模型中的encoder及权重","uri":"/posts/%E5%8D%95%E7%8B%AC%E6%8F%90%E5%8F%96xx%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84encoder%E5%8F%8A%E6%9D%83%E9%87%8D/"},{"categories":["大模型"],"content":"\r","date":"2024-08-11 14:01:06","objectID":"/posts/%E5%8D%95%E7%8B%AC%E6%8F%90%E5%8F%96xx%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84encoder%E5%8F%8A%E6%9D%83%E9%87%8D/:1:0","tags":["LLM"],"title":"单独提取XX大模型中的encoder及权重","uri":"/posts/%E5%8D%95%E7%8B%AC%E6%8F%90%E5%8F%96xx%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84encoder%E5%8F%8A%E6%9D%83%E9%87%8D/"},{"categories":["大模型"],"content":"1. 先去huggingface和github下载它的 模型权重文件 和 Codebase 代码文件，然后看 Codebase 里面的 modeling.py文件，找到你想要的 Class Encoder，找到整体架构初始化这个 Encoder的代码，看它怎么初始化的（可以配合hg的演示代码）。这里拿QWen-Audio，提取它的 Audio Encoder 举例子：\r​按照官方提供的加载方法加载模型，并将整个模型的权重state_dict输出到一个txt文件到本地，目的是查看想提取模块的名字，也可能有model.safetensors.index.json这些文件，里面也写清楚了，就不用这一步。 tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True) model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True) # 保存权重到本地 with open('param.txt', 'w') as file: import sys sys.stdout = file for name, param in model.name_parameters(): print(name) print(param) sys.stdout = sys.__stdout__ state_dict = model.state_dict ","date":"2024-08-11 14:01:06","objectID":"/posts/%E5%8D%95%E7%8B%AC%E6%8F%90%E5%8F%96xx%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84encoder%E5%8F%8A%E6%9D%83%E9%87%8D/:1:1","tags":["LLM"],"title":"单独提取XX大模型中的encoder及权重","uri":"/posts/%E5%8D%95%E7%8B%AC%E6%8F%90%E5%8F%96xx%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84encoder%E5%8F%8A%E6%9D%83%E9%87%8D/"},{"categories":["大模型"],"content":"2. 下一步就是选出这些对应的需要保存的权重，保存到本地.pt\rselected_keys = ['audio'] selected_params = {key: value for key, value in state_dict.items() for selected_key in selected_keys if selected_key in key} # 保存 torch,save(selected_params, 'QWen-Audio-Encoder.pt') ","date":"2024-08-11 14:01:06","objectID":"/posts/%E5%8D%95%E7%8B%AC%E6%8F%90%E5%8F%96xx%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84encoder%E5%8F%8A%E6%9D%83%E9%87%8D/:1:2","tags":["LLM"],"title":"单独提取XX大模型中的encoder及权重","uri":"/posts/%E5%8D%95%E7%8B%AC%E6%8F%90%E5%8F%96xx%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84encoder%E5%8F%8A%E6%9D%83%E9%87%8D/"},{"categories":["大模型"],"content":"3. 下面就是要修改 .pt 参数文件中可能不对应 Class Encoder 的权重参数名字\rnew_state_dict = {} # 遍历原来的 state_dict 并去掉 'yyy.xxx' 前缀 for k, v in state_dict.items(): new_key = k.replace('yyyyyyy.xxxxx.', '') new_state_dict[new_key] = v # 同样是保存，这个 final.pt 就是这个开源模型训练好的最终权重参数了 torch,save(selected_params, 'QWen-Audio-Encoder-final.pt') ","date":"2024-08-11 14:01:06","objectID":"/posts/%E5%8D%95%E7%8B%AC%E6%8F%90%E5%8F%96xx%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84encoder%E5%8F%8A%E6%9D%83%E9%87%8D/:1:3","tags":["LLM"],"title":"单独提取XX大模型中的encoder及权重","uri":"/posts/%E5%8D%95%E7%8B%AC%E6%8F%90%E5%8F%96xx%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84encoder%E5%8F%8A%E6%9D%83%E9%87%8D/"},{"categories":["大模型"],"content":"4. 后面的事情就简单了！初始化 Encoder 并且加载这个单独模块开源后的权重参数。Qwen-Audio用的是Whisper2-large，可以用官方的modeling或者用Qwen-Audio提供的modeling类并加载权重。\raudio_config = {'xxxx'} # 这里是你想要提取的 encoder，config 看官方配置 # AudioEncoder 用提供好的类 audio_encoder = AudioEncoder(**audio_config) audio_encoder.load_state_dict(torch.load('path/to/new.pt'), strict=False) ","date":"2024-08-11 14:01:06","objectID":"/posts/%E5%8D%95%E7%8B%AC%E6%8F%90%E5%8F%96xx%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84encoder%E5%8F%8A%E6%9D%83%E9%87%8D/:1:4","tags":["LLM"],"title":"单独提取XX大模型中的encoder及权重","uri":"/posts/%E5%8D%95%E7%8B%AC%E6%8F%90%E5%8F%96xx%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84encoder%E5%8F%8A%E6%9D%83%E9%87%8D/"},{"categories":["大模型"],"content":"5. 中间可以输出 state_dict 到txt文件中查看是否加载正确，当加载完以后，就是正常的使用 encoder 了，来拿到想要的 features\raudio_info = tokenizer.process_audio(wav_path) audios = audio_info['input_audios'] audio_span_tokens = audio_info['audio_span_tokens'] input_audio_lengths = audio_info['input_audio_lengths'] ","date":"2024-08-11 14:01:06","objectID":"/posts/%E5%8D%95%E7%8B%AC%E6%8F%90%E5%8F%96xx%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84encoder%E5%8F%8A%E6%9D%83%E9%87%8D/:1:5","tags":["LLM"],"title":"单独提取XX大模型中的encoder及权重","uri":"/posts/%E5%8D%95%E7%8B%AC%E6%8F%90%E5%8F%96xx%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84encoder%E5%8F%8A%E6%9D%83%E9%87%8D/"},{"categories":["大模型"],"content":"\r","date":"2024-08-10 14:22:38","objectID":"/posts/ubuntu%E5%AE%89%E8%A3%85ffmpeg7%E4%BB%A5%E5%8F%8A%E9%81%87%E5%88%B0%E7%9A%84%E9%94%99%E8%AF%AF/:1:0","tags":["LLM"],"title":"Ubuntu安装ffmpeg7以及遇到的错误","uri":"/posts/ubuntu%E5%AE%89%E8%A3%85ffmpeg7%E4%BB%A5%E5%8F%8A%E9%81%87%E5%88%B0%E7%9A%84%E9%94%99%E8%AF%AF/"},{"categories":["大模型"],"content":"ubuntu 离线安装 ffmpeg 7.0\r先下载ffmpeg7压缩包 wget https://ffmpeg.org/releases/ffmpeg-7.0.tar.gz 接着安装相应依赖 sudo apt-get install yasm 然后完成安装 tar -zxvf ffmpeg-7.0.tar.gz # 安装 cd ffmpeg-7.0 ./configure --enable-shared --prefix=/usr/local/ffmpeg sudo make \u0026\u0026 sudo make install ","date":"2024-08-10 14:22:38","objectID":"/posts/ubuntu%E5%AE%89%E8%A3%85ffmpeg7%E4%BB%A5%E5%8F%8A%E9%81%87%E5%88%B0%E7%9A%84%E9%94%99%E8%AF%AF/:1:1","tags":["LLM"],"title":"Ubuntu安装ffmpeg7以及遇到的错误","uri":"/posts/ubuntu%E5%AE%89%E8%A3%85ffmpeg7%E4%BB%A5%E5%8F%8A%E9%81%87%E5%88%B0%E7%9A%84%E9%94%99%E8%AF%AF/"},{"categories":["大模型"],"content":"下面是安装过程中遇到的错误\r1. ffmpeg查看版本报错error while loading shared libraries: libavdevice.so.59 61\r解决方法： vim /etc/ld.so.conf 在最后一行加入/usr/local/ffmpeg/lib，具体看自己的安装目录 刷新配置ldconfig即可 ","date":"2024-08-10 14:22:38","objectID":"/posts/ubuntu%E5%AE%89%E8%A3%85ffmpeg7%E4%BB%A5%E5%8F%8A%E9%81%87%E5%88%B0%E7%9A%84%E9%94%99%E8%AF%AF/:1:2","tags":["LLM"],"title":"Ubuntu安装ffmpeg7以及遇到的错误","uri":"/posts/ubuntu%E5%AE%89%E8%A3%85ffmpeg7%E4%BB%A5%E5%8F%8A%E9%81%87%E5%88%B0%E7%9A%84%E9%94%99%E8%AF%AF/"}]